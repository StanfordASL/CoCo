{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time\n",
    "import random\n",
    "import pickle, os\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from free_flyer.free_flyer import FreeFlyer\n",
    "from free_flyer.utils import *\n",
    "from solvers.mlopt_ff import MLOPT_FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train/test data\n",
    "prob = FreeFlyer() #use default config, pass different config file oth.\n",
    "config_fn = './free_flyer/config/default.p'\n",
    "\n",
    "config_file = open(config_fn,'rb')\n",
    "dataset_name, _, _ = pickle.load(config_file); config_file.close()\n",
    "\n",
    "relative_path = os.getcwd()\n",
    "dataset_fn = relative_path + '/free_flyer/data/' + dataset_name\n",
    "\n",
    "train_file = open(dataset_fn+'/train.p','rb')\n",
    "# p_train, x_train, u_train, y_train, c_train, times_train = pickle.load(train_file)\n",
    "train_data = pickle.load(train_file)\n",
    "train_file.close()\n",
    "x_train = train_data[1]\n",
    "y_train = train_data[3]\n",
    "\n",
    "test_file = open(dataset_fn+'/test.p','rb')\n",
    "# p_test, x_test, u_test, y_test, c_test, times_test = pickle.load(test_file)\n",
    "test_data = pickle.load(test_file)\n",
    "p_test, x_test, u_test, y_test, c_test, times_test = test_data\n",
    "test_file.close()\n",
    "\n",
    "n_test = x_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacky utility functions for passing network weights around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copies all network weights from target NN to source NN except for last layer\n",
    "def copy_shared_params(target, source):\n",
    "    ff_depth = len(target.ff_layers)-1\n",
    "    last_layer_names = ['ff_layers.{}.weight'.format(ff_depth), 'ff_layers.{}.bias'.format(ff_depth)]\n",
    "    source_params, target_params = source.named_parameters(), target.named_parameters()\n",
    "    target_params_dict = dict(target_params)\n",
    "    for name, param in source_params:\n",
    "        if name in last_layer_names:\n",
    "            continue\n",
    "        if name in target_params_dict:\n",
    "            target_params_dict[name].data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates target NN parameters using list of weights in source_data except for last layer\n",
    "def copy_all_but_last(target, source_data):\n",
    "    target_params = target.named_parameters()\n",
    "    target_params_dict = dict(target_params)\n",
    "    \n",
    "    idx = 0\n",
    "    for ii in range(len(target.conv_layers)):\n",
    "        target_params_dict['conv_layers.{}.weight'.format(ii)].data.copy_(source_data[idx].data)\n",
    "        idx+=1\n",
    "        target_params_dict['conv_layers.{}.bias'.format(ii)].data.copy_(source_data[idx].data)\n",
    "        idx+=1\n",
    "\n",
    "    for ii in range(len(target.ff_layers)-1):\n",
    "        target_params_dict['ff_layers.{}.weight'.format(ii)].data.copy_(source_data[idx].data)\n",
    "        idx+=1\n",
    "        target_params_dict['ff_layers.{}.bias'.format(ii)].data.copy_(source_data[idx].data)\n",
    "        idx+=1\n",
    "\n",
    "        # Updates target NN parameters using list of weights in source_data for last layer\n",
    "def copy_last(target, source_data):\n",
    "    target_params = target.named_parameters()\n",
    "    target_params_dict = dict(target_params)\n",
    "\n",
    "    ff_depth = len(target.ff_layers)-1\n",
    "    target_params_dict['ff_layers.{}.weight'.format(ff_depth)].data.copy_(source_data[-2].data)\n",
    "    target_params_dict['ff_layers.{}.bias'.format(ff_depth)].data.copy_(source_data[-1].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MLOPT network with pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading presaved classifier model from models/mloptff_free_flyer_20200716_0708.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNNet(\n",
       "  (conv_activation): ReLU()\n",
       "  (ff_activation): ReLU()\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): Conv2d(16, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (2): Conv2d(16, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "  )\n",
       "  (ff_layers): ModuleList(\n",
       "    (0): Linear(in_features=260, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): Linear(in_features=128, out_features=458, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = 'free_flyer'\n",
    "prob_features = ['x0', 'obstacles_map']\n",
    "\n",
    "mlopt_cnn = MLOPT_FF(system, prob, prob_features)\n",
    "\n",
    "n_features = 4\n",
    "mlopt_cnn.construct_strategies(n_features, train_data)\n",
    "\n",
    "device_id = 1 # use -1 for CPU\n",
    "mlopt_cnn.setup_network(device_id=device_id)\n",
    "\n",
    "fn_saved = 'models/mloptff_free_flyer_20200716_0708.pt'   # New spaced out dataset\n",
    "mlopt_cnn.load_network(fn_saved)\n",
    "\n",
    "mlopt_cnn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feas_model = deepcopy(mlopt_cnn.model)\n",
    "mlopt_model = mlopt_cnn.model\n",
    "copy_shared_params(feas_model, mlopt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_lr = 1e-3\n",
    "update_lr = 1e-3\n",
    "num_train = train_data[0]['x0'].shape[0]\n",
    "writer = SummaryWriter(\"runs/warm_start_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab training params\n",
    "BATCH_SIZE = mlopt_cnn.training_params['BATCH_SIZE']\n",
    "TRAINING_ITERATIONS = mlopt_cnn.training_params['TRAINING_ITERATIONS']\n",
    "TRAINING_ITERATIONS = 5\n",
    "BATCH_SIZE = mlopt_cnn.training_params['BATCH_SIZE']\n",
    "CHECKPOINT_AFTER = mlopt_cnn.training_params['CHECKPOINT_AFTER']\n",
    "CCHECKPOINT_AFTER = int(1000)\n",
    "SAVEPOINT_AFTER = mlopt_cnn.training_params['SAVEPOINT_AFTER']\n",
    "SAVEPOINT_AFTER = int(200)\n",
    "TEST_BATCH_SIZE = mlopt_cnn.training_params['TEST_BATCH_SIZE']\n",
    "\n",
    "NUM_META_PROBLEMS = np.maximum(int(np.ceil(BATCH_SIZE / mlopt_cnn.problem.n_obs)), 10)\n",
    "UPDATE_STEP = 1\n",
    "\n",
    "model = mlopt_model\n",
    "model.to(device=mlopt_cnn.device)\n",
    "\n",
    "params = train_data[0]\n",
    "X = mlopt_cnn.features[:mlopt_cnn.problem.n_obs*mlopt_cnn.num_train]\n",
    "X_cnn = np.zeros((BATCH_SIZE, 3,mlopt_cnn.problem.H,mlopt_cnn.problem.W))\n",
    "Y = mlopt_cnn.labels[:mlopt_cnn.problem.n_obs*mlopt_cnn.num_train,0]\n",
    "\n",
    "training_loss = torch.nn.CrossEntropyLoss()\n",
    "meta_opt = torch.optim.Adam(model.parameters(), lr=meta_lr, weight_decay=0.00001)\n",
    "\n",
    "itr = 1\n",
    "for epoch in range(TRAINING_ITERATIONS):  # loop over the dataset multiple times\n",
    "    t0 = time.time()\n",
    "    running_loss = 0.0\n",
    "    rand_idx = list(np.arange(0,X.shape[0]-1))\n",
    "    random.shuffle(rand_idx)\n",
    "\n",
    "    # Sample all data points\n",
    "    indices = [rand_idx[ii * BATCH_SIZE:(ii + 1) * BATCH_SIZE] for ii in range((len(rand_idx) + BATCH_SIZE - 1) // BATCH_SIZE)]\n",
    "\n",
    "    for ii,idx in enumerate(indices):\n",
    "\n",
    "        # fast_weights are network weights for feas_model with descent steps taken\n",
    "        fast_weights = list(feas_model.parameters())\n",
    "        for _ in range(UPDATE_STEP):\n",
    "            inner_loss = torch.zeros(1)\n",
    "\n",
    "            # Update feas_model to be using fast_weights\n",
    "            copy_last(feas_model, fast_weights)\n",
    "            copy_all_but_last(feas_model, fast_weights)\n",
    "\n",
    "            for __ in range(NUM_META_PROBLEMS):\n",
    "                idx_val = np.random.randint(0,num_train)\n",
    "\n",
    "                params = train_data[0]\n",
    "\n",
    "                prob_params = {}\n",
    "                for k in params:\n",
    "                    prob_params[k] = params[k][mlopt_cnn.cnn_features_idx[idx_val][0]]\n",
    "\n",
    "                ff_inputs_inner = torch.from_numpy(mlopt_cnn.problem.construct_features(prob_params, mlopt_cnn.prob_features))\n",
    "                ff_inputs_inner = Variable(ff_inputs_inner.repeat(mlopt_cnn.problem.n_obs,1)).float().to(device=mlopt_cnn.device)\n",
    "\n",
    "                X_cnn_inner = np.zeros((mlopt_cnn.problem.n_obs, 3, mlopt_cnn.problem.H, mlopt_cnn.problem.W))\n",
    "                for ii_obs in range(mlopt_cnn.problem.n_obs):\n",
    "                    X_cnn_inner[ii_obs] = mlopt_cnn.problem.construct_cnn_features(prob_params, \\\n",
    "                                    mlopt_cnn.prob_features, \\\n",
    "                                    ii_obs=ii_obs)\n",
    "                cnn_inputs_inner = Variable(torch.from_numpy(X_cnn_inner)).float().to(device=mlopt_cnn.device)\n",
    "\n",
    "                scores = mlopt_model_copy(cnn_inputs_inner, ff_inputs_inner).detach().cpu().numpy()\n",
    "                class_labels = np.argmax(scores, axis=1)\n",
    "\n",
    "                y_guess = np.zeros((4*mlopt_cnn.problem.n_obs, mlopt_cnn.problem.N-1))\n",
    "                for cl_ii,cl in enumerate(class_labels):\n",
    "                    cl_idx = np.where(mlopt_cnn.labels[:,0] == cl)[0][0]\n",
    "                    y_obs = mlopt_cnn.labels[cl_idx,1:]\n",
    "                    y_guess[4*cl_ii:4*(cl_ii+1)] = np.reshape(y_obs, (4, mlopt_cnn.problem.N-1))\n",
    "\n",
    "                # Sometimes Mosek fails, so try again with Gurobi\n",
    "                try:\n",
    "                    prob_success = mlopt_cnn.problem.solve_pinned(prob_params, y_guess, solver=cp.MOSEK)[0]\n",
    "                except:\n",
    "                    prob_success = mlopt_cnn.problem.solve_pinned(prob_params, y_guess, solver=cp.GUROBI)[0]\n",
    "\n",
    "                losses = torch.zeros(8,1)\n",
    "\n",
    "                feas_scores = feas_model(cnn_inputs_inner, ff_inputs_inner)\n",
    "                for ii_obs in range(mlopt_cnn.problem.n_obs):\n",
    "                    losses[ii_obs] = feas_scores[ii_obs,class_labels[ii_obs]]\n",
    "\n",
    "                margin = 10.\n",
    "                if prob_success:\n",
    "                    # If problem feasible, push scores to positive value\n",
    "                    inner_loss += torch.relu(margin - torch.sum(losses))\n",
    "                else:\n",
    "                    # If problem infeasible, push scores to negative value \n",
    "                    inner_loss += torch.relu(margin + torch.sum(losses))\n",
    "\n",
    "            # Descent step on feas_model network weights\n",
    "            inner_loss /= float(NUM_META_PROBLEMS)\n",
    "            grad = torch.autograd.grad(inner_loss, fast_weights)\n",
    "            fast_weights = list(map(lambda p: p[1] - update_lr * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "        # Pass inner loop weights to MLOPT classifier (except last layer)\n",
    "        copy_all_but_last(mlopt_model_copy, fast_weights)\n",
    "\n",
    "        ff_inputs = Variable(torch.from_numpy(X[idx,:])).float().to(device=mlopt_cnn.device)\n",
    "        labels = Variable(torch.from_numpy(Y[idx])).long().to(device=mlopt_cnn.device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        X_cnn = np.zeros((len(idx), 3,mlopt_cnn.problem.H,mlopt_cnn.problem.W))\n",
    "        for idx_ii, idx_val in enumerate(idx):\n",
    "            prob_params = {}\n",
    "            for k in params:\n",
    "                prob_params[k] = params[k][mlopt_cnn.cnn_features_idx[idx_val][0]]\n",
    "            X_cnn[idx_ii] = mlopt_cnn.problem.construct_cnn_features(prob_params, mlopt_cnn.prob_features, ii_obs=mlopt_cnn.cnn_features_idx[idx_val][1])\n",
    "        cnn_inputs = Variable(torch.from_numpy(X_cnn)).float().to(device=mlopt_cnn.device)\n",
    "        outputs = model(cnn_inputs, ff_inputs)\n",
    "\n",
    "        loss = training_loss(outputs, labels).float().to(device=mlopt_cnn.device)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        class_guesses = torch.argmax(outputs,1)\n",
    "        accuracy = torch.mean(torch.eq(class_guesses,labels).float())\n",
    "        loss.backward()\n",
    "        meta_opt.step()\n",
    "        meta_opt.zero_grad() # zero the parameter gradients\n",
    "\n",
    "        # Update feas_model weights\n",
    "        copy_last(feas_model, [fw.detach() for fw in fast_weights])\n",
    "        copy_shared_params(feas_model, model)\n",
    "\n",
    "        if itr % CHECKPOINT_AFTER == 0:\n",
    "            rand_idx = list(np.arange(0,X.shape[0]-1))\n",
    "            random.shuffle(rand_idx)\n",
    "            test_inds = rand_idx[:TEST_BATCH_SIZE]\n",
    "            ff_inputs = Variable(torch.from_numpy(X[test_inds,:])).float().to(device=mlopt_cnn.device)\n",
    "            labels = Variable(torch.from_numpy(Y[test_inds])).long().to(device=mlopt_cnn.device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            X_cnn = np.zeros((len(test_inds), 3,mlopt_cnn.problem.H,mlopt_cnn.problem.W))\n",
    "            for idx_ii, idx_val in enumerate(test_inds):\n",
    "                prob_params = {}\n",
    "                for k in params:\n",
    "                    prob_params[k] = params[k][mlopt_cnn.cnn_features_idx[idx_val][0]]\n",
    "                X_cnn[idx_ii] = mlopt_cnn.problem.construct_cnn_features(prob_params, mlopt_cnn.prob_features, ii_obs=mlopt_cnn.cnn_features_idx[idx_val][1])\n",
    "            cnn_inputs = Variable(torch.from_numpy(X_cnn)).float().to(device=mlopt_cnn.device)\n",
    "            outputs = model(cnn_inputs, ff_inputs)\n",
    "\n",
    "            loss = training_loss(outputs, labels).float().to(device=mlopt_cnn.device)\n",
    "            class_guesses = torch.argmax(outputs,1)\n",
    "            accuracy = torch.mean(torch.eq(class_guesses,labels).float())\n",
    "            print(\"loss:   \"+str(loss.item())+\",   acc:  \"+str(accuracy.item()))\n",
    "\n",
    "        if itr % SAVEPOINT_AFTER == 0:\n",
    "            writer.add_scalar('Loss/train', running_loss / float(SAVEPOINT_AFTER), itr)\n",
    "            running_loss = 0.\n",
    "\n",
    "        itr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlopt",
   "language": "python",
   "name": "mlopt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
