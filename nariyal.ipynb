{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "\n",
    "from free_flyer.free_flyer import FreeFlyer\n",
    "from free_flyer.utils import *\n",
    "import pickle, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train/test data\n",
    "prob = FreeFlyer() #use default config, pass different config file oth.\n",
    "config_fn = './free_flyer/config/default.p'\n",
    "\n",
    "config_file = open(config_fn,'rb')\n",
    "dataset_name, _, _ = pickle.load(config_file); config_file.close()\n",
    "\n",
    "relative_path = os.getcwd()\n",
    "dataset_fn = relative_path + '/free_flyer/data/' + dataset_name\n",
    "\n",
    "train_file = open(dataset_fn+'/train.p','rb')\n",
    "# p_train, x_train, u_train, y_train, c_train, times_train = pickle.load(train_file)\n",
    "train_data = pickle.load(train_file)\n",
    "train_file.close()\n",
    "x_train = train_data[1]\n",
    "y_train = train_data[3]\n",
    "\n",
    "test_file = open(dataset_fn+'/test.p','rb')\n",
    "# p_test, x_test, u_test, y_test, c_test, times_test = pickle.load(test_file)\n",
    "test_data = pickle.load(test_file)\n",
    "p_test, x_test, u_test, y_test, c_test, times_test = test_data\n",
    "test_file.close()\n",
    "\n",
    "n_test = x_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solvers.mlopt_ff import MLOPT_FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458\n"
     ]
    }
   ],
   "source": [
    "system = 'free_flyer'\n",
    "prob_features = ['x0', 'obstacles_map']\n",
    "\n",
    "mlopt_cnn = MLOPT_FF(system, prob, prob_features)\n",
    "\n",
    "n_features = 4\n",
    "mlopt_cnn.construct_strategies(n_features, train_data)\n",
    "print(mlopt_cnn.n_strategies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNet(\n",
       "  (conv_activation): ReLU()\n",
       "  (ff_activation): ReLU()\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): Conv2d(16, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (2): Conv2d(16, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "  )\n",
       "  (ff_layers): ModuleList(\n",
       "    (0): Linear(in_features=260, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): Linear(in_features=128, out_features=458, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlopt_cnn.setup_network()\n",
    "\n",
    "fn_saved = 'None' # horizon of 11\n",
    "mlopt_cnn.load_network(fn_saved)\n",
    "\n",
    "mlopt_cnn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNet(\n",
       "  (conv_activation): ReLU()\n",
       "  (ff_activation): ReLU()\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): Conv2d(16, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (2): Conv2d(16, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "  )\n",
       "  (ff_layers): ModuleList(\n",
       "    (0): Linear(in_features=260, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (3): Linear(in_features=128, out_features=458, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlopt_cnn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_shared_params(source, target, depth=3):\n",
    "    last_layer_names = ['ff_layers.{}.weight'.format(depth), 'ff_layers.{}.bias'.format(depth)]\n",
    "    source_params, target_params = source.named_parameters(), target.named_parameters()\n",
    "    target_params_dict = dict(target_params)\n",
    "    for name, param in source_params:\n",
    "        if name in last_layer_names:\n",
    "            continue\n",
    "        if name in target_params_dict:\n",
    "            target_params_dict[name].data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_all_but_last(target, source_data, depth=3):\n",
    "    target_params = target.named_parameters()\n",
    "    target_params_dict = dict(target_params)\n",
    "\n",
    "    target_params_dict['conv_layers.0.weight'].data.copy_(source_data[0].data)\n",
    "    target_params_dict['conv_layers.0.bias'].data.copy_(source_data[1].data)\n",
    "\n",
    "    target_params_dict['conv_layers.1.weight'].data.copy_(source_data[2].data)\n",
    "    target_params_dict['conv_layers.1.bias'].data.copy_(source_data[3].data)\n",
    "\n",
    "    target_params_dict['conv_layers.2.weight'].data.copy_(source_data[4].data)\n",
    "    target_params_dict['conv_layers.2.bias'].data.copy_(source_data[5].data)\n",
    "\n",
    "    target_params_dict['ff_layers.0.weight'].data.copy_(source_data[6].data)\n",
    "    target_params_dict['ff_layers.0.bias'].data.copy_(source_data[7].data)\n",
    "\n",
    "    target_params_dict['ff_layers.1.weight'].data.copy_(source_data[8].data)\n",
    "    target_params_dict['ff_layers.1.bias'].data.copy_(source_data[9].data)\n",
    "\n",
    "    target_params_dict['ff_layers.2.weight'].data.copy_(source_data[10].data)\n",
    "    target_params_dict['ff_layers.2.bias'].data.copy_(source_data[11].data)\n",
    "\n",
    "def copy_last(target, source_data, depth=3):\n",
    "    target_params = target.named_parameters()\n",
    "    target_params_dict = dict(target_params)\n",
    "\n",
    "    target_params_dict['ff_layers.3.weight'].data.copy_(source_data[12].data)\n",
    "    target_params_dict['ff_layers.3.bias'].data.copy_(source_data[13].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "feas_model = deepcopy(mlopt_cnn.model)\n",
    "mlopt_model_copy = deepcopy(mlopt_cnn.model)\n",
    "\n",
    "copy_shared_params(mlopt_model_copy, feas_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_train = train_data[0]['x0'].shape[0]\n",
    "\n",
    "# np.random.seed(12)\n",
    "# idxs = np.random.randint(0,num_train)\n",
    "\n",
    "# inputs = torch.zeros((mlopt_cnn.problem.n_obs,mlopt_cnn.n_y))\n",
    "# inputs = Variable(torch.from_numpy(mlopt_cnn.features[idxs,:])).float().to(device=mlopt_cnn.device)\n",
    "# outputs = Variable(torch.from_numpy(mlopt_cnn.labels[idxs,:])).long().to(device=mlopt_cnn.device)\n",
    "\n",
    "# copy_shared_params(feas_model, mlopt_model_copy)\n",
    "# if 'obstacles_map' in prob_features:\n",
    "#     params = train_data[0]\n",
    "#     X_cnn = np.zeros((len(idxs), 3, mlopt_cnn.problem.H, mlopt_cnn.problem.W))\n",
    "#     for idx_ii, idx_val in enumerate(idx):\n",
    "#         prob_params = {}\n",
    "#         for k in params:\n",
    "#             prob_params[k] = params[k][mlopt_cnn.cnn_features_idx[idx_val][0]]\n",
    "#         X_cnn[idx_ii] = mlopt_cnn.problem.construct_cnn_features(prob_params, \\\n",
    "#                         mlopt_cnn.prob_features, \\\n",
    "#                         ii_obs=mlopt_cnn.cnn_features_idx[idx_val][1])\n",
    "#         cnn_inputs = Variable(torch.from_numpy(X_cnn)).float().to(device=mlopt_cnn.device)\n",
    "\n",
    "# scores = mlopt_model_copy(cnn_inputs, inputs).detach().cpu().numpy()\n",
    "# class_labels = np.argmax(scores, axis=1)\n",
    "\n",
    "# y_guesses = np.zeros((len(idxs), mlopt_cnn.n_y))\n",
    "\n",
    "# for ii,cl in enumerate(class_labels):\n",
    "#     idx = np.where(mlopt_cnn.labels[:,0] == cl)[0][0]\n",
    "#     y_guesses[ii,:] = mlopt_cnn.labels[idx,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_lr = 1e-3\n",
    "update_lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = train_data[0]['x0'].shape[0]\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "loss = torch.zeros(1)\n",
    "\n",
    "for _ in range(10):\n",
    "    idx = np.random.randint(0,num_train)\n",
    "\n",
    "#     copy_shared_params(feas_model, mlopt_model_copy)\n",
    "    if 'obstacles_map' in prob_features:\n",
    "        params = train_data[0]\n",
    "\n",
    "        prob_params = {}\n",
    "        for k in params:\n",
    "            prob_params[k] = params[k][mlopt_cnn.cnn_features_idx[idx_val][0]]\n",
    "\n",
    "        ff_inputs = torch.from_numpy(mlopt_cnn.problem.construct_features(prob_params, mlopt_cnn.prob_features))\n",
    "        ff_inputs = Variable(ff.repeat(mlopt_cnn.problem.n_obs,1)).float().to(device=mlopt_cnn.device)\n",
    "\n",
    "        X_cnn = np.zeros((mlopt_cnn.problem.n_obs, 3, mlopt_cnn.problem.H, mlopt_cnn.problem.W))\n",
    "        for ii_obs in range(mlopt_cnn.problem.n_obs):\n",
    "            X_cnn[ii_obs] = mlopt_cnn.problem.construct_cnn_features(prob_params, \\\n",
    "                            mlopt_cnn.prob_features, \\\n",
    "                            ii_obs=ii_obs)\n",
    "        cnn_inputs = Variable(torch.from_numpy(X_cnn)).float().to(device=mlopt_cnn.device)\n",
    "\n",
    "    scores = mlopt_model_copy(cnn_inputs, ff_inputs).detach().cpu().numpy()\n",
    "    class_labels = np.argmax(scores, axis=1)\n",
    "\n",
    "    y_guesses = np.zeros((mlopt_cnn.problem.n_obs, mlopt_cnn.n_y))\n",
    "    y_guess = np.zeros((4*mlopt_cnn.problem.n_obs, mlopt_cnn.problem.N-1))\n",
    "\n",
    "    for ii,cl in enumerate(class_labels):\n",
    "        idx = np.where(mlopt_cnn.labels[:,0] == cl)[0][0]\n",
    "        y_obs = mlopt_cnn.labels[idx,1:]\n",
    "        y_guesses[ii,:] = y_obs\n",
    "        y_guess[4*ii:4*(ii+1)] = np.reshape(y_obs, (4, mlopt_cnn.problem.N-1))\n",
    "\n",
    "    prob_success = mlopt_cnn.problem.solve_pinned(prob_params, y_guess, solver=cp.MOSEK)[0]\n",
    "\n",
    "    margin = 10.\n",
    "    losses = torch.zeros(8,1)\n",
    "\n",
    "    feas_scores = feas_model(cnn_inputs, ff_inputs)\n",
    "    for ii_obs in range(mlopt_cnn.problem.n_obs):\n",
    "        losses[ii_obs] = feas_scores[ii_obs,class_labels[ii_obs]]\n",
    "\n",
    "#     print('worked' if prob_success else 'failed')\n",
    "    if prob_success:\n",
    "        loss += torch.relu(margin - torch.sum(losses))\n",
    "    else:\n",
    "        loss += torch.relu(margin + torch.sum(losses))\n",
    "\n",
    "grad = torch.autograd.grad(loss, feas_model.parameters())\n",
    "fast_weights = list(map(lambda p: p[1] - update_lr * p[0], zip(grad, feas_model.parameters())))\n",
    "\n",
    "copy_all_but_last(mlopt_model_copy, fast_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab training params\n",
    "BATCH_SIZE = mlopt_cnn.training_params['BATCH_SIZE']\n",
    "TRAINING_ITERATIONS = mlopt_cnn.training_params['TRAINING_ITERATIONS']\n",
    "TRAINING_ITERATIONS = 1\n",
    "BATCH_SIZE = mlopt_cnn.training_params['BATCH_SIZE']\n",
    "BATCH_SIZE = 256\n",
    "CHECKPOINT_AFTER = mlopt_cnn.training_params['CHECKPOINT_AFTER']\n",
    "SAVEPOINT_AFTER = mlopt_cnn.training_params['SAVEPOINT_AFTER']\n",
    "TEST_BATCH_SIZE = mlopt_cnn.training_params['TEST_BATCH_SIZE']\n",
    "\n",
    "NUM_META_PROBLEMS = np.maximum(int(BATCH_SIZE / mlopt_cnn.problem.n_obs), 50)\n",
    "\n",
    "model = mlopt_model_copy\n",
    "model.to(device=mlopt_cnn.device)\n",
    "\n",
    "X = mlopt_cnn.features[:mlopt_cnn.problem.n_obs*mlopt_cnn.num_train]\n",
    "X_cnn = None\n",
    "if 'obstacles_map' in mlopt_cnn.prob_features:\n",
    "# X_cnn = self.cnn_features[:self.problem.n_obs*self.num_train]\n",
    "# TODO(acauligi)\n",
    "    params = train_data[0]\n",
    "    X_cnn = np.zeros((BATCH_SIZE, 3,mlopt_cnn.problem.H,mlopt_cnn.problem.W))\n",
    "    Y = mlopt_cnn.labels[:mlopt_cnn.problem.n_obs*mlopt_cnn.num_train,0]\n",
    "\n",
    "training_loss = torch.nn.CrossEntropyLoss()\n",
    "meta_opt = torch.optim.Adam(model.parameters(), lr=meta_lr, weight_decay=0.00001)\n",
    "\n",
    "\n",
    "itr = 1\n",
    "for epoch in range(TRAINING_ITERATIONS):  # loop over the dataset multiple times\n",
    "    t0 = time.time()\n",
    "    running_loss = 0.0\n",
    "    rand_idx = list(np.arange(0,X.shape[0]-1))\n",
    "    random.shuffle(rand_idx)\n",
    "\n",
    "    # Sample all data points\n",
    "    indices = [rand_idx[ii * BATCH_SIZE:(ii + 1) * BATCH_SIZE] for ii in range((len(rand_idx) + BATCH_SIZE - 1) // BATCH_SIZE)]\n",
    "\n",
    "    for ii,idx in enumerate(indices):\n",
    "        # zero the parameter gradients\n",
    "        meta_opt.zero_grad()\n",
    "\n",
    "        inner_loss = torch.zeros(1)\n",
    "        for _ in range(NUM_META_PROBLEMS):\n",
    "            idx_val = np.random.randint(0,num_train)\n",
    "\n",
    "        #     copy_shared_params(feas_model, mlopt_model_copy)\n",
    "            if 'obstacles_map' in prob_features:\n",
    "                params = train_data[0]\n",
    "\n",
    "                prob_params = {}\n",
    "                for k in params:\n",
    "                    prob_params[k] = params[k][mlopt_cnn.cnn_features_idx[idx_val][0]]\n",
    "\n",
    "                ff_inputs = torch.from_numpy(mlopt_cnn.problem.construct_features(prob_params, mlopt_cnn.prob_features))\n",
    "                ff_inputs = Variable(ff.repeat(mlopt_cnn.problem.n_obs,1)).float().to(device=mlopt_cnn.device)\n",
    "\n",
    "                X_cnn = np.zeros((mlopt_cnn.problem.n_obs, 3, mlopt_cnn.problem.H, mlopt_cnn.problem.W))\n",
    "                for ii_obs in range(mlopt_cnn.problem.n_obs):\n",
    "                    X_cnn[ii_obs] = mlopt_cnn.problem.construct_cnn_features(prob_params, \\\n",
    "                                    mlopt_cnn.prob_features, \\\n",
    "                                    ii_obs=ii_obs)\n",
    "                cnn_inputs = Variable(torch.from_numpy(X_cnn)).float().to(device=mlopt_cnn.device)\n",
    "\n",
    "            scores = mlopt_model_copy(cnn_inputs, ff_inputs).detach().cpu().numpy()\n",
    "            class_labels = np.argmax(scores, axis=1)\n",
    "\n",
    "            y_guesses = np.zeros((mlopt_cnn.problem.n_obs, mlopt_cnn.n_y))\n",
    "            y_guess = np.zeros((4*mlopt_cnn.problem.n_obs, mlopt_cnn.problem.N-1))\n",
    "\n",
    "            for ii,cl in enumerate(class_labels):\n",
    "                cl_idx = np.where(mlopt_cnn.labels[:,0] == cl)[0][0]\n",
    "                y_obs = mlopt_cnn.labels[cl_idx,1:]\n",
    "                y_guesses[ii,:] = y_obs\n",
    "                y_guess[4*ii:4*(ii+1)] = np.reshape(y_obs, (4, mlopt_cnn.problem.N-1))\n",
    "\n",
    "            prob_success = mlopt_cnn.problem.solve_pinned(prob_params, y_guess, solver=cp.MOSEK)[0]\n",
    "\n",
    "            margin = 10.\n",
    "            losses = torch.zeros(8,1)\n",
    "\n",
    "            feas_scores = feas_model(cnn_inputs, ff_inputs)\n",
    "            for ii_obs in range(mlopt_cnn.problem.n_obs):\n",
    "                losses[ii_obs] = feas_scores[ii_obs,class_labels[ii_obs]]\n",
    "\n",
    "        #     print('worked' if prob_success else 'failed')\n",
    "            if prob_success:\n",
    "                inner_loss += torch.relu(margin - torch.sum(losses))\n",
    "            else:\n",
    "                inner_loss += torch.relu(margin + torch.sum(losses))\n",
    "\n",
    "        grad = torch.autograd.grad(inner_loss, feas_model.parameters())\n",
    "        fast_weights = list(map(lambda p: p[1] - update_lr * p[0], zip(grad, feas_model.parameters())))\n",
    "\n",
    "        copy_all_but_last(mlopt_model_copy, fast_weights)\n",
    "\n",
    "        ff_inputs = Variable(torch.from_numpy(X[idx,:])).float().to(device=mlopt_cnn.device)\n",
    "        labels = Variable(torch.from_numpy(Y[idx])).long().to(device=mlopt_cnn.device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = None\n",
    "        if 'obstacles_map' in mlopt_cnn.prob_features:\n",
    "            X_cnn = np.zeros((len(idx), 3,mlopt_cnn.problem.H,mlopt_cnn.problem.W))\n",
    "            for idx_ii, idx_val in enumerate(idx):\n",
    "                prob_params = {}\n",
    "                for k in params:\n",
    "                    prob_params[k] = params[k][mlopt_cnn.cnn_features_idx[idx_val][0]]\n",
    "                X_cnn[idx_ii] = mlopt_cnn.problem.construct_cnn_features(prob_params, mlopt_cnn.prob_features, ii_obs=mlopt_cnn.cnn_features_idx[idx_val][1])\n",
    "            cnn_inputs = Variable(torch.from_numpy(X_cnn)).float().to(device=mlopt_cnn.device)\n",
    "            outputs = model(cnn_inputs, ff_inputs)\n",
    "        else:\n",
    "            outputs = model(ff_inputs)\n",
    "\n",
    "        loss = training_loss(outputs, labels).float().to(device=mlopt_cnn.device)\n",
    "        class_guesses = torch.argmax(outputs,1)\n",
    "        accuracy = torch.mean(torch.eq(class_guesses,labels).float())\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm(model.parameters(),0.1)\n",
    "        meta_opt.step()\n",
    "\n",
    "        copy_last(feas_model, [fw.detach() for fw in fast_weights])\n",
    "        copy_shared_params(feas_model, model)\n",
    "\n",
    "        itr += 1\n",
    "        if ii % 100 == 0:\n",
    "            print(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlopt",
   "language": "python",
   "name": "mlopt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
